import gradio as gr
import torch
import torch.nn.functional as F
import clip
from torchvision import transforms
import os
from PIL import Image
import json
from pathlib import Path
import tempfile
import numpy as np
import time
from typing import List, Dict, Any
import argparse
import glob
import cv2
from transformers.models.clip.modeling_clip import CLIPVisionModelWithProjection
from diffusers import AutoencoderKL
import sys
sys.path.append('/home/humw/Codes/FaceOff/gradio/ip_adapter')
from ip_adapter.resampler import Resampler
from ip_adapter.ip_adapter import ImageProjModel

# è®¾ç½®éšæœºç§å­
seed = 1
torch.manual_seed(seed)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(seed)

class ImageProtector:
    def __init__(self):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model_dict = {}
        self.image_proj_model_dict = {}
        self.eot_trans_list = []
        
    def setup_models(self, model_types: List[str], precision: str = "fp16"):
        """è®¾ç½®æ¨¡å‹"""
        if precision == "fp32":
            torch_dtype = torch.float32
        elif precision == "fp16":
            torch_dtype = torch.float16
        elif precision == "bf16":
            torch_dtype = torch.bfloat16
        else:
            raise ValueError("precision must be one of [fp32, fp16, bf16]")
            
        self.model_dict = {}
        self.image_proj_model_dict = {}
        
        # æ ¹æ®æ¨¡å‹ç±»å‹è®¾ç½®é»˜è®¤çš„æ¨¡å‹è·¯å¾„
        model_paths = {
            'clip': '/data1/humw/Pretrains/ViT-B-32.pt',
            'photomaker': '/data1/humw/Pretrains/photomaker-v1.bin',
            'ipadapter': '/data1/humw/Pretrains/IP-Adapter/models/image_encoder',
            'ipadapter-plus': '/data1/humw/Pretrains/IP-Adapter/sdxl_models/image_encoder',
            'face_diffuser': '/data1/humw/Pretrains/clip-vit-large-patch14'
        }
        
        for model_type in model_types:
            if model_type in ['clip', 'ViT-B32', 'ViT-B16', 'ViT-L14']:
                model, _ = clip.load(model_paths['clip'], device=self.device)
                model.to(torch_dtype)
                self.model_dict[model_type] = model
                
            elif model_type == 'photomaker':
                # ç®€åŒ–ç‰ˆçš„PhotoMakeræ¨¡å‹åŠ è½½
                try:
                    from photomaker_clip import PhotoMakerIDEncoder
                    model = PhotoMakerIDEncoder()
                    # è¿™é‡Œåº”è¯¥åŠ è½½é¢„è®­ç»ƒæƒé‡ï¼Œç®€åŒ–å¤„ç†
                    model.to(self.device, dtype=torch_dtype)
                    self.model_dict[model_type] = model
                except:
                    print(f"Warning: Could not load {model_type} model")
                    
            elif model_type in ['ipadapter', 'ipadapter-plus']:
                try:
                    model = CLIPVisionModelWithProjection.from_pretrained(
                        model_paths['ipadapter']).to(self.device, dtype=torch_dtype)
                    self.model_dict[model_type] = model
                    # ç®€åŒ–æŠ•å½±æ¨¡å‹
                    self.image_proj_model_dict[model_type] = None
                except:
                    print(f"Warning: Could not load {model_type} model")
                    
            elif model_type == 'face_diffuser':
                try:
                    # ç®€åŒ–ç‰ˆçš„FaceDiffuseræ¨¡å‹
                    class SimpleFaceDiffuser(torch.nn.Module):
                        def __init__(self):
                            super().__init__()
                            self.encoder = CLIPVisionModelWithProjection.from_pretrained("openai/clip-vit-base-patch32")
                            
                        def forward(self, x):
                            return self.encoder(x).image_embeds
                    
                    model = SimpleFaceDiffuser().to(self.device, dtype=torch_dtype)
                    self.model_dict[model_type] = model
                except:
                    print(f"Warning: Could not load {model_type} model")
        
        return True
    
    def setup_transforms(self, eot_trans_types: List[str], resample_interpolation: str = "BILINEAR"):
        """è®¾ç½®å˜æ¢"""
        if resample_interpolation == 'BILINEAR':
            resample_interpolation_enum = transforms.InterpolationMode.BILINEAR
        else:
            resample_interpolation_enum = transforms.InterpolationMode.BICUBIC
            
        self.eot_trans_list = []
        
        for trans_type in eot_trans_types:
            train_aug_for_clip = [
                transforms.Resize(224, interpolation=resample_interpolation_enum),
                transforms.CenterCrop(224),
            ]
            
            tensorize_and_normalize = [
                transforms.Normalize([0.5 * 255]*3, [0.5 * 255]*3),
            ]
            
            if trans_type == 'gau':
                gau_filter = transforms.GaussianBlur(kernel_size=7)
                defense_transform = [gau_filter]
            elif trans_type == 'hflip':
                hflip = transforms.RandomHorizontalFlip(p=0.5)
                defense_transform = [hflip]
            elif trans_type == 'none':
                defense_transform = []
            elif trans_type == 'gau-hflip':
                gau_filter = transforms.GaussianBlur(kernel_size=7)
                hflip = transforms.RandomHorizontalFlip(p=0.5)
                defense_transform = [gau_filter, hflip]
            else:
                defense_transform = []
                
            trans_for_clip = train_aug_for_clip + defense_transform + tensorize_and_normalize
            trans_for_clip = transforms.Compose(trans_for_clip)
            self.eot_trans_list.append(trans_for_clip)
            
        return True
    
    def load_target_data(self, target_type: str, input_size: int):
        """åŠ è½½ç›®æ ‡æ•°æ®"""
        # åˆ›å»ºç®€å•çš„ç›®æ ‡å›¾åƒ
        if target_type == 'max':
            # åˆ›å»ºéšæœºçº¹ç†ä½œä¸ºç›®æ ‡
            target_array = np.random.randint(0, 255, (input_size, input_size, 3), dtype=np.uint8)
        elif target_type == 'gray':
            target_array = np.full((input_size, input_size, 3), 128, dtype=np.uint8)
        elif target_type == 'mist':
            # åˆ›å»ºé›¾çŠ¶æ•ˆæœ
            target_array = np.random.randint(200, 255, (input_size, input_size, 3), dtype=np.uint8)
        elif target_type == 'colored_mist':
            # åˆ›å»ºå½©è‰²é›¾çŠ¶æ•ˆæœ
            target_array = np.random.randint(150, 255, (input_size, input_size, 3), dtype=np.uint8)
        else:  # yingbu or default
            # åˆ›å»ºäº¬å‰§è„¸è°±é£æ ¼çš„åº•è‰²
            base_color = [200, 150, 100]  # æ©™é»„è‰²è°ƒ
            target_array = np.full((input_size, input_size, 3), base_color, dtype=np.uint8)
        
        target_image = Image.fromarray(target_array)
        target_tensor = torch.from_numpy(np.array(target_image)).permute(2, 0, 1).unsqueeze(0).float()
        return target_tensor
    
    def pgd_ensemble_attack(self, perturbed_data, origin_data, alpha, eps, attack_num, 
                           target_data, loss_choice, w):
        """PGDé›†æˆæ”»å‡»ç®—æ³• - ä»æ–‡æ¡£2ç§»æ¤"""
        with torch.no_grad():
            origin_data.requires_grad_(False)
            eot_trans_target_data_list = []
            eot_trans_origin_data_list = []
            # print("è®¡ç®—EOTå˜æ¢ä¸‹çš„ç›®æ ‡æ•°æ®å’ŒåŸå§‹æ•°æ®")
            for trans in self.eot_trans_list:
                tran_target_data = trans(target_data)
                tran_origin_data = trans(origin_data)
                eot_trans_target_data_list.append(tran_target_data)
                eot_trans_origin_data_list.append(tran_origin_data)
            
            d_type = perturbed_data.dtype
            # åˆå§‹æ‰°åŠ¨
            # print("æ·»åŠ åˆå§‹æ‰°åŠ¨")
            perturbed_data = (perturbed_data + (torch.rand(*perturbed_data.shape)*2*eps-eps).to(perturbed_data.device)).to(d_type)
            
            target_embeds_dict = {}
            origin_embeds_dict = {}
            
            # è®¡ç®—ç›®æ ‡åµŒå…¥å’ŒåŸå§‹åµŒå…¥
            # print("è®¡ç®—ç›®æ ‡åµŒå…¥å’ŒåŸå§‹åµŒå…¥")
            for k in self.model_dict.keys():
                model_type = k
                model = self.model_dict[k]
                eot_trans_target_embeds_list = []
                eot_trans_origin_embeds_list = []
                # print(f"Processing model: {model_type}")
                for i in range(len(self.eot_trans_list)):
                    # print(f"  EOT transform {i+1}/{len(self.eot_trans_list)}")
                    tran_target_data = eot_trans_target_data_list[i]
                    tran_origin_data = eot_trans_origin_data_list[i]
                    # print(type(tran_target_data), tran_target_data.shape)
                    # print(type(tran_origin_data), tran_origin_data.shape)
                    # import pdb; pdb.set_trace()
                    if model_type in ['clip', 'ViT-B32', 'ViT-B16', 'ViT-L14']:
                        target_embeds = model.encode_image(tran_target_data) # 
                        origin_embeds = model.encode_image(tran_origin_data)
                    elif model_type == 'photomaker':
                        target_embeds = model(tran_target_data)
                        origin_embeds = model(tran_origin_data)
                    elif model_type in ['ipadapter', 'ipadapter-plus']:
                        target_embeds = model(tran_target_data).image_embeds
                        origin_embeds = model(tran_origin_data).image_embeds
                    elif model_type == 'face_diffuser':
                        target_embeds = model(tran_target_data.unsqueeze(0))
                        origin_embeds = model(tran_origin_data.unsqueeze(0))
                    else:
                        # # é»˜è®¤ä½¿ç”¨CLIP-likeç¼–ç 
                        # target_embeds = model.encode_image(tran_target_data) if hasattr(model, 'encode_image') else model(tran_target_data)
                        # origin_embeds = model.encode_image(tran_origin_data) if hasattr(model, 'encode_image') else model(tran_origin_data)
                        raise ValueError(f"Unknown model type: {model_type}")
                    
                    eot_trans_target_embeds_list.append(target_embeds)
                    eot_trans_origin_embeds_list.append(origin_embeds)
                
                target_embeds_dict[k] = eot_trans_target_embeds_list
                origin_embeds_dict[k] = eot_trans_origin_embeds_list
        
        Loss_dict = {}
        
        # PGDæ”»å‡»è¿­ä»£
        # print("å¼€å§‹PGDæ”»å‡»è¿­ä»£")
        for epoch in range(attack_num):
            perturbed_data.requires_grad_()
            eot_trans_perturbed_data_list = []
            
            for trans in self.eot_trans_list:
                tran_perturbed_data = trans(perturbed_data)
                eot_trans_perturbed_data_list.append(tran_perturbed_data)
            
            Loss_x_ = []
            Loss_d_ = []
            # print("è®¡ç®—æŸå¤±")
            for k in self.model_dict.keys():
                model_type = k
                model = self.model_dict[k]
                loss_x_list = []
                loss_d_list = []
                
                for i in range(len(self.eot_trans_list)):
                    tran_perturbed_data = eot_trans_perturbed_data_list[i]
                    wi = 1  # æƒé‡å‚æ•°
                    
                    if model_type in ['clip', 'ViT-B32', 'ViT-B16', 'ViT-L14']:
                        adv_embeds = model.encode_image(tran_perturbed_data)
                    elif model_type == 'photomaker':
                        adv_embeds = model(tran_perturbed_data)
                    elif model_type in ['ipadapter', 'ipadapter-plus']:
                        adv_embeds = model(tran_perturbed_data).image_embeds
                    elif model_type == 'face_diffuser':
                        adv_embeds = model(tran_perturbed_data.unsqueeze(0))
                    else:
                        adv_embeds = model.encode_image(tran_perturbed_data) if hasattr(model, 'encode_image') else model(tran_perturbed_data)
                    
                    target_embeds_list = target_embeds_dict[k]
                    origin_embeds_list = origin_embeds_dict[k]
                    target_embeds = target_embeds_list[i]
                    origin_embeds = origin_embeds_list[i]
                    
                    if loss_choice == 'mse':
                        Loss_x = wi * F.mse_loss(adv_embeds, target_embeds, reduction="mean")
                        Loss_d = -F.mse_loss(adv_embeds, origin_embeds, reduction="mean")
                    else:  # cosine
                        Loss_x = -F.cosine_similarity(adv_embeds, target_embeds, -1).mean()
                        Loss_d = F.cosine_similarity(adv_embeds, origin_embeds, -1).mean()
                    
                    loss_x_list.append(Loss_x)
                    loss_d_list.append(Loss_d)
                
                # EOTå˜æ¢ä¸‹çš„å¹³å‡æŸå¤±
                mean_Loss_x = torch.stack(loss_x_list).mean()
                mean_Loss_d = torch.stack(loss_d_list).mean()
                Loss_x_.append(mean_Loss_x)
                Loss_d_.append(mean_Loss_d)
            
            # ç»„åˆæŸå¤±
            Loss_x_ = torch.stack(Loss_x_).view(1, len(self.model_dict.keys()))
            Loss_d_ = torch.stack(Loss_d_).view(1, len(self.model_dict.keys()))
            Loss_ = (1 - w) * Loss_x_ + w * Loss_d_
            Loss_ = Loss_.mean()
            
            Loss_dict[epoch] = [Loss_.item(), Loss_x_.mean().item(), Loss_d_.mean().item()]
            
            if epoch % 50 == 0:
                print(f"Epoch {epoch}: Loss = {Loss_.item():.4f}")
            
            # è®¡ç®—æ¢¯åº¦å¹¶æ›´æ–°
            # print("è®¡ç®—æ¢¯åº¦å¹¶æ›´æ–°æ‰°åŠ¨")
            grad = torch.autograd.grad(Loss_, perturbed_data)[0]
            adv_perturbed_data = perturbed_data - alpha * grad.sign()
            
            # æŠ•å½±åˆ°epsçƒå†…
            # print("æŠ•å½±æ‰°åŠ¨åˆ°epsçƒå†…")
            et = torch.clamp(adv_perturbed_data - origin_data, min=-eps, max=+eps)
            perturbed_data = torch.clamp(origin_data + et, min=0, max=255).detach().clone()
        
        return perturbed_data.cpu(), Loss_dict
    
    def protect_image(self, input_image: Image.Image, parameters: Dict[str, Any]) -> Image.Image:
        """ä¿æŠ¤å•å¼ å›¾åƒ - ä½¿ç”¨é›†æˆçš„PGDæ”»å‡»ç®—æ³•"""
        # try:
        # è½¬æ¢è¾“å…¥å›¾åƒä¸ºtensor
        # print("è½¬æ¢è¾“å…¥å›¾åƒä¸ºtensor")
        input_array = np.array(input_image).astype(np.uint8)
        input_tensor = torch.from_numpy(input_array).permute(2, 0, 1).unsqueeze(0).float()
        
        # è®¾ç½®æ¨¡å‹å’Œå˜æ¢
        # print("è®¾ç½®æ¨¡å‹å’Œå˜æ¢")
        precision = "fp32"
        if precision == "fp32":
            torch_dtype = torch.float32
        elif precision == "fp16":
            torch_dtype = torch.float16
        elif precision == "bf16":
            torch_dtype = torch.bfloat16
        else:
            raise ValueError("precision must be one of [fp32, fp16, bf16]")
        self.setup_models(parameters['model_types'], precision)
        self.setup_transforms(parameters['eot_trans_types'], parameters['resample_interpolation'])
        
        # åŠ è½½ç›®æ ‡æ•°æ®
        # print("åŠ è½½ç›®æ ‡æ•°æ®")
        target_tensor = self.load_target_data(parameters['target_type'], parameters['input_size'])
        
        # å‡†å¤‡æ•°æ®
        origin_data = input_tensor.detach().clone().to(self.device).requires_grad_(False).to(torch_dtype)
        perturbed_data = input_tensor.to(self.device).requires_grad_(True).to(torch_dtype)
        target_data = target_tensor.to(self.device).requires_grad_(False).to(torch_dtype)
        
        # åº”ç”¨PGDé›†æˆæ”»å‡»
        # print("åº”ç”¨PGDé›†æˆæ”»å‡»")
        alpha_val = parameters['alpha'] * 255  # ç¼©æ”¾æ­¥é•¿
        eps_val = parameters['eps']  # å™ªå£°é¢„ç®—

        protected_tensor, loss_dict = self.pgd_ensemble_attack(
            perturbed_data=perturbed_data,
            origin_data=origin_data,
            alpha=alpha_val,
            eps=eps_val,
            attack_num=parameters['attack_num'],
            target_data=target_data,
            loss_choice=parameters['loss_choice'],
            w=parameters['w']
        )
        for epoch, losses in loss_dict.items():
            print(f"Final Epoch {epoch}: Total Loss = {losses[0]:.4f}, Target Loss = {losses[1]:.4f}, Deviation Loss = {losses[2]:.4f}")
        # è½¬æ¢å›PILå›¾åƒ
        protected_array = protected_tensor.squeeze(0).permute(1, 2, 0).cpu().numpy().astype(np.uint8)
        protected_image = Image.fromarray(protected_array)
        
        return protected_image
        
        # except Exception as e:
        #     print(f"Error in protect_image: {e}")
        #     # å‡ºé”™æ—¶è¿”å›åŸå§‹å›¾åƒ
        #     return input_image

def create_protector_interface():
    """åˆ›å»ºGradioç•Œé¢"""
    
    protector = ImageProtector()
    
    def protect_single_image(input_image, 
                           model_types, 
                           attack_num, 
                           loss_choice,
                           w,
                           alpha,
                           eps,
                           input_size,
                           eot_trans_types,
                           resample_interpolation,
                           target_type):
        """ä¿æŠ¤å•å¼ å›¾åƒ"""
        
        # è®¾ç½®å‚æ•°
        parameters = {
            'model_types': model_types,
            'attack_num': attack_num,
            'loss_choice': loss_choice,
            'w': w,
            'alpha': alpha,
            'eps': eps,
            'input_size': input_size,
            'eot_trans_types': eot_trans_types,
            'resample_interpolation': resample_interpolation,
            'target_type': target_type
        }
        
        if input_image is None:
            return None, "è¯·å…ˆä¸Šä¼ å›¾åƒ"
        
        try:
            # è°ƒæ•´å›¾åƒå¤§å°
            if input_size != input_image.size[0]:
                input_image = input_image.resize((input_size, input_size), Image.BILINEAR)
            
            # ä¿æŠ¤å›¾åƒ
            protected_image = protector.protect_image(input_image, parameters)
            return protected_image, "å›¾åƒä¿æŠ¤å®Œæˆï¼"
            
        except Exception as e:
            return None, f"å¤„ç†å›¾åƒæ—¶å‡ºé”™: {str(e)}"
    
    def protect_batch_images(input_folder, 
                           model_types, 
                           attack_num, 
                           loss_choice,
                           w,
                           alpha,
                           eps,
                           input_size,
                           eot_trans_types,
                           resample_interpolation,
                           target_type):
        """ä¿æŠ¤æ‰¹é‡å›¾åƒ"""
        
        if input_folder is None:
            return None, "è¯·å…ˆé€‰æ‹©æ–‡ä»¶å¤¹"
        # print(f"Processing folder: {input_folder}")
        try:
            # å¤„ç†æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰å›¾åƒ
            output_dir = tempfile.mkdtemp()
            image_files = []
            
            if isinstance(input_folder, list):
                # Gradioè¿”å›æ–‡ä»¶åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ä¸´æ—¶æ–‡ä»¶å¯¹è±¡
                for file_obj in input_folder:
                    if file_obj.name.lower().endswith(('.jpg', '.jpeg', '.png')):
                        image_files.append(file_obj.name)
            else:
                # å¦‚æœæ˜¯å­—ç¬¦ä¸²è·¯å¾„ï¼ˆå¤‡ç”¨ï¼‰
                for ext in ['*.jpg', '*.JPG', '*.png', '*.PNG', '*.jpeg', '*.JPEG']:
                    image_files.extend(glob.glob(f"{input_folder}/**/{ext}", recursive=True))
            
            if not image_files:
                return None, "æ–‡ä»¶å¤¹ä¸­æ²¡æœ‰æ‰¾åˆ°å›¾åƒæ–‡ä»¶"
            
            processed_images = []
            parameters = {
                'model_types': model_types,
                'attack_num': attack_num,
                'loss_choice': loss_choice,
                'w': w,
                'alpha': alpha,
                'eps': eps,
                'input_size': input_size,
                'eot_trans_types': eot_trans_types,
                'resample_interpolation': resample_interpolation,
                'target_type': target_type
            }
            
            for i, image_file in enumerate(image_files[:8]):  # é™åˆ¶å¤„ç†æ•°é‡
                try:
                    img = Image.open(image_file)
                    if input_size != img.size[0]:
                        img = img.resize((input_size, input_size), Image.BILINEAR)
                    # print("å¼€å§‹ä¿æŠ¤å›¾åƒ")
                    protected_img = protector.protect_image(img, parameters)
                    processed_images.append(protected_img)
                    
                except Exception as e:
                    print(f"å¤„ç†å›¾åƒ {image_file} æ—¶å‡ºé”™: {e}")
            
            if not processed_images:
                return None, "æ²¡æœ‰æˆåŠŸå¤„ç†ä»»ä½•å›¾åƒ"
            
            return processed_images, f"æˆåŠŸå¤„ç† {len(processed_images)} å¼ å›¾åƒ"
            
        except Exception as e:
            return None, f"å¤„ç†æ–‡ä»¶å¤¹æ—¶å‡ºé”™: {str(e)}"
    
    # åˆ›å»ºç•Œé¢
    with gr.Blocks(title="å›¾åƒä¿æŠ¤ç³»ç»Ÿ", theme="soft") as interface:
        gr.Markdown("# ğŸ›¡ï¸ å›¾åƒä¿æŠ¤ç³»ç»Ÿ")
        gr.Markdown("ä½¿ç”¨å¯¹æŠ—æ€§æ”»å‡»æŠ€æœ¯ä¿æŠ¤æ‚¨çš„å›¾åƒå…é­æœªç»æˆæƒçš„AIæ¨¡å‹ä½¿ç”¨")
        
        with gr.Tab("å•å¼ å›¾åƒä¿æŠ¤"):
            with gr.Row():
                with gr.Column():
                    input_image = gr.Image(label="ä¸Šä¼ å¾…ä¿æŠ¤å›¾åƒ", type="pil")
                    
                    with gr.Accordion("æ”»å‡»å‚æ•°", open=False):
                        model_types = gr.CheckboxGroup(
                            choices=["clip", "ipadapter", "photomaker", "face_diffuser"],
                            value=["clip", "photomaker"],
                            label="ç›®æ ‡æ¨¡å‹ç±»å‹"
                        )
                        
                        attack_num = gr.Slider(
                            minimum=10, maximum=500, value=100, step=10,
                            label="æ”»å‡»è¿­ä»£æ¬¡æ•°"
                        )
                        
                        loss_choice = gr.Radio(
                            choices=["cosine", "mse"], value="cosine",
                            label="æŸå¤±å‡½æ•°ç±»å‹"
                        )
                        
                        w = gr.Slider(
                            minimum=0.0, maximum=1.0, value=0.5, step=0.1,
                            label="æƒé‡å‚æ•° w"
                        )
                        
                        alpha = gr.Slider(
                            minimum=0.001, maximum=0.1, value=0.005, step=0.001,
                            label="æ­¥é•¿ alpha"
                        )
                        
                        eps = gr.Slider(
                            minimum=1, maximum=32, value=16, step=1,
                            label="å™ªå£°é¢„ç®— eps"
                        )
                    
                    with gr.Accordion("å›¾åƒå‚æ•°", open=False):
                        input_size = gr.Slider(
                            minimum=224, maximum=1024, value=512, step=32,
                            label="è¾“å…¥å›¾åƒå°ºå¯¸"
                        )
                        
                        eot_trans_types = gr.CheckboxGroup(
                            choices=["none", "gau", "hflip", "gau-hflip"],
                            value=["none"],
                            label="EOTå˜æ¢ç±»å‹"
                        )
                        
                        resample_interpolation = gr.Radio(
                            choices=["BILINEAR", "BICUBIC"], value="BILINEAR",
                            label="é‡é‡‡æ ·æ’å€¼æ–¹æ³•"
                        )
                        
                        target_type = gr.Radio(
                            choices=["max", "yingbu", "mist", "colored_mist", "gray"],
                            value="max",
                            label="ç›®æ ‡å›¾åƒç±»å‹"
                        )
                    
                    protect_btn = gr.Button("ğŸ›¡ï¸ ä¿æŠ¤å›¾åƒ", variant="primary")
                
                with gr.Column():
                    output_image = gr.Image(label="ä¿æŠ¤åçš„å›¾åƒ", interactive=False)
                    status_text = gr.Textbox(label="çŠ¶æ€ä¿¡æ¯", interactive=False)
        
        with gr.Tab("æ‰¹é‡å›¾åƒä¿æŠ¤"):
            with gr.Row():
                with gr.Column():
                    input_folder = gr.File(
                        label="é€‰æ‹©å›¾åƒæ–‡ä»¶å¤¹",
                        file_count="directory"
                    )
                    
                    # å¤ç”¨ç›¸åŒçš„å‚æ•°
                    with gr.Accordion("æ”»å‡»å‚æ•°", open=False):
                        batch_model_types = gr.CheckboxGroup(
                            choices=["clip", "ipadapter", "photomaker", "face_diffuser"],
                            value=["clip", "photomaker"],
                            label="ç›®æ ‡æ¨¡å‹ç±»å‹"
                        )
                        
                        batch_attack_num = gr.Slider(
                            minimum=10, maximum=500, value=100, step=10,
                            label="æ”»å‡»è¿­ä»£æ¬¡æ•°"
                        )
                        
                        batch_loss_choice = gr.Radio(
                            choices=["cosine", "mse"], value="cosine",
                            label="æŸå¤±å‡½æ•°ç±»å‹"
                        )
                        
                        batch_w = gr.Slider(
                            minimum=0.0, maximum=1.0, value=0.5, step=0.1,
                            label="æƒé‡å‚æ•° w"
                        )
                        
                        batch_alpha = gr.Slider(
                            minimum=0.001, maximum=0.1, value=0.005, step=0.001,
                            label="æ­¥é•¿ alpha"
                        )
                        
                        batch_eps = gr.Slider(
                            minimum=1, maximum=32, value=16, step=1,
                            label="å™ªå£°é¢„ç®— eps"
                        )
                    
                    with gr.Accordion("å›¾åƒå‚æ•°", open=False):
                        batch_input_size = gr.Slider(
                            minimum=224, maximum=1024, value=512, step=32,
                            label="è¾“å…¥å›¾åƒå°ºå¯¸"
                        )
                        
                        batch_eot_trans_types = gr.CheckboxGroup(
                            choices=["none", "gau", "hflip", "gau-hflip"],
                            value=["none"],
                            label="EOTå˜æ¢ç±»å‹"
                        )
                        
                        batch_resample_interpolation = gr.Radio(
                            choices=["BILINEAR", "BICUBIC"], value="BILINEAR",
                            label="é‡é‡‡æ ·æ’å€¼æ–¹æ³•"
                        )
                        
                        batch_target_type = gr.Radio(
                            choices=["max", "yingbu", "mist", "colored_mist", "gray"],
                            value="max",
                            label="ç›®æ ‡å›¾åƒç±»å‹"
                        )
                    
                    batch_protect_btn = gr.Button("ğŸ›¡ï¸ æ‰¹é‡ä¿æŠ¤å›¾åƒ", variant="primary")
                
                with gr.Column():
                    batch_output_gallery = gr.Gallery(
                        label="ä¿æŠ¤åçš„å›¾åƒ",
                        show_label=True,
                        elem_id="gallery",
                        columns=2,
                        height="auto"
                    )
                    batch_status_text = gr.Textbox(label="çŠ¶æ€ä¿¡æ¯", interactive=False)
        
        # ç»‘å®šäº‹ä»¶
        protect_btn.click(
            fn=protect_single_image,
            inputs=[
                input_image, model_types, attack_num, loss_choice, w, alpha, eps,
                input_size, eot_trans_types, resample_interpolation, target_type
            ],
            outputs=[output_image, status_text]
        )
        
        batch_protect_btn.click(
            fn=protect_batch_images,
            inputs=[
                input_folder, batch_model_types, batch_attack_num, batch_loss_choice,
                batch_w, batch_alpha, batch_eps, batch_input_size, batch_eot_trans_types,
                batch_resample_interpolation, batch_target_type
            ],
            outputs=[batch_output_gallery, batch_status_text]
        )
        
        # æ·»åŠ è¯´æ˜
        with gr.Accordion("ä½¿ç”¨è¯´æ˜", open=False):
            gr.Markdown("""
            ## å›¾åƒä¿æŠ¤ç³»ç»Ÿä½¿ç”¨æŒ‡å—
            
            ### åŠŸèƒ½è¯´æ˜
            - **å•å¼ å›¾åƒä¿æŠ¤**: ä¸Šä¼ å•å¼ å›¾åƒè¿›è¡Œä¿æŠ¤å¤„ç†
            - **æ‰¹é‡å›¾åƒä¿æŠ¤**: ä¸Šä¼ åŒ…å«å¤šå¼ å›¾åƒçš„æ–‡ä»¶å¤¹è¿›è¡Œæ‰¹é‡å¤„ç†
            
            ### æ ¸å¿ƒç®—æ³•
            æœ¬ç³»ç»Ÿä½¿ç”¨åŸºäºPGD(Projected Gradient Descent)çš„é›†æˆæ”»å‡»ç®—æ³•ï¼Œé€šè¿‡æ·»åŠ äººçœ¼éš¾ä»¥å¯Ÿè§‰çš„å¯¹æŠ—æ€§å™ªå£°ï¼Œ
            ä½¿AIæ¨¡å‹æ— æ³•æ­£ç¡®è¯†åˆ«å’Œå¤„ç†å—ä¿æŠ¤çš„å›¾åƒã€‚
            
            ### å‚æ•°è¯´æ˜
            - **ç›®æ ‡æ¨¡å‹ç±»å‹**: é€‰æ‹©è¦é˜²å¾¡çš„AIæ¨¡å‹ç±»å‹
            - **æ”»å‡»è¿­ä»£æ¬¡æ•°**: å¯¹æŠ—æ€§æ”»å‡»çš„è¿­ä»£æ¬¡æ•°ï¼Œå€¼è¶Šå¤§æ•ˆæœè¶Šå¥½ä½†è€—æ—¶è¶Šé•¿
            - **æŸå¤±å‡½æ•°ç±»å‹**: é€‰æ‹©ç”¨äºä¼˜åŒ–çš„æŸå¤±å‡½æ•°(ä½™å¼¦ç›¸ä¼¼åº¦æˆ–å‡æ–¹è¯¯å·®)
            - **æƒé‡å‚æ•° w**: å¹³è¡¡ç›®æ ‡æŸå¤±å’Œåå·®æŸå¤±çš„æƒé‡
            - **æ­¥é•¿ alpha**: æ¯æ¬¡æ”»å‡»è¿­ä»£çš„æ­¥é•¿å¤§å°
            - **å™ªå£°é¢„ç®— eps**: å…è®¸æ·»åŠ çš„æœ€å¤§å™ªå£°é‡
            - **EOTå˜æ¢ç±»å‹**: æœŸæœ›è¿‡åº¦å˜æ¢çš„ç±»å‹ï¼Œç”¨äºå¢å¼ºæ”»å‡»çš„é²æ£’æ€§
            
            ### ä½¿ç”¨å»ºè®®
            1. é¦–å…ˆä½¿ç”¨é»˜è®¤å‚æ•°å°è¯•å•å¼ å›¾åƒä¿æŠ¤
            2. æ ¹æ®æ•ˆæœè°ƒæ•´å‚æ•°(å»ºè®®ä¼˜å…ˆè°ƒæ•´epså’Œattack_num)
            3. ç¡®è®¤æ•ˆæœæ»¡æ„åè¿›è¡Œæ‰¹é‡å¤„ç†
            """)
    
    return interface

if __name__ == "__main__":
    # åˆ›å»ºå¹¶å¯åŠ¨Gradioç•Œé¢
    interface = create_protector_interface()
    interface.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=True
    )